Trackformer: Self-Supervised Music Source Separation using performer

Performer + MixIT + Audio MAE = Compact, context-aware music de-mixer

â¸»

ğŸ” Overview

MixPerformer is a self-supervised framework that swaps the CNN-based ConvTasNet separator in MixIT for a linear-attention Performer Transformer and jump-starts learning with Audio Masked Autoencoder (Audio MAE) pre-training. By combining globalâ€context attention with label-free MixIT training, MixPerformer separates up to four musical stems while cutting training time and GPU memory. ï¿¼

â¸»

ğŸš¨ Motivation
	â€¢	Local-only CNN limits â€“ ConvTasNet excels at short-range features but struggles with long rhythmic or harmonic structures typical of music. ï¿¼
	â€¢	Memory & scalability â€“ ConvTasNet breaks on long (30 s) segments with CUDA OOM, blocking high-fidelity separation. ï¿¼

â¸»

ğŸ§  Key Ideas

Component	Role
Performer encoder (12 layers, FAVOR+)	Linear O(L Â· r) attention captures long-range music context. ï¿¼
Audio MAE pre-training	Learns robust global audio tokens before MixIT fine-tuning. ï¿¼
MixIT loss	Matches any permutation of 8 soft-mask outputs to two mixturesâ€”no labels required. ï¿¼
Auxiliary Diversity & Sparsity losses	Reduce mask overlap and encourage energy concentration without ground truth. ï¿¼


â¸»

ğŸ§ª Datasets & Protocols
	â€¢	MOISESDB (â‰ˆ 500 h) â€“ four-stem songs only. ï¿¼
	â€¢	Segment lengths : 3 s, 5 s, 10 s, 30 s. ï¿¼
	â€¢	Train / Val split : 80 % / 20 %. ï¿¼

â¸»

âš™ï¸ Implementation Details
	â€¢	Masks (N) : 8â€ƒâ€¢â€ƒBatch : 4â€ƒâ€¢â€ƒLR : 1 e-4 â†’ 1 e-5 cosineâ€ƒâ€¢â€ƒOpt : AdamW (wd = 0.01) ï¿¼
	â€¢	Audio MAE : 30 % masking, 256 random features. ï¿¼
	â€¢	Hardware : single Tesla V100 32 GB. ï¿¼

â¸»

ğŸ† Results

Segment	ConvTasNet SI-SNR (dB)	Performer SI-SNR (dB)	Speed-up
3 s	â€“ 9.11	+1.66	1.36 Ã— faster (17.1 â†’ 12.6 min/epoch) ï¿¼
5 s	â€“ 8.52	+2.00	1.27 Ã— faster ï¿¼
10 s	â€“ 9.73	+2.33	1.22 Ã— faster
30 s	OOM	+3.52	âœ… trains (46 min/epoch)

	â€¢	+3 â€“ 13 dB absolute gain over ConvTasNet across lengths. ï¿¼
	â€¢	No OOM on 30 s thanks to linear attention. ï¿¼

Ablation â€“ Removing Audio MAE boosts short-segment SI-SNR but loses memory savings (OOM at 30 s). ï¿¼

â¸»

ğŸ“Œ Why MixPerformer?

Feature	MixPerformer	ConvTasNet
Global context	âœ… Performer attention	âŒ Local conv
Self-supervised	âœ… MixIT	âœ…
Handles 30 s audio	âœ… Fits GPU	âŒ OOM
Training time	â†“ 20â€“30 %	â€“
Mask interpretability	âœ… Diversity + Sparsity losses	âš ï¸ Overlapping masks

Key findings summarised in the paper confirm performance, efficiency and global-context advantages. ï¿¼

â¸»

ğŸ“‰ Current Limitations
	â€¢	Small corpus (â‰ˆ 10 h) â‡’ over-fitting; val/test SI-SNR can drop below 0 dB. ï¿¼
	â€¢	Linear attention compression may lose fine details; up-sampling refinements planned. ï¿¼
	â€¢	Fixed output count (N=8); dynamic source estimation is future work. ï¿¼

â¸»

ğŸ”® Roadmap
	1.	Scale pre-training to 100 â€“ 1000 h diverse music. ï¿¼
	2.	Multi-metric eval (SDR, PESQ, SI-SDR). ï¿¼
	3.	Stereo / 5.1 & real-time extensions for live performance. ï¿¼

â¸»

Built with â¤ï¸ by Team 4 (Yunyoung Lee et al.).
